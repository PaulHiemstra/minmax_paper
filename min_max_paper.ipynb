{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "respected-irish",
   "metadata": {},
   "source": [
    "# An AI agent plays tic-tac-toe (part 1): building the opponent to play against\n",
    "*Where we build a brute force minimax tree search which will serve as the opponent to our reinforcement learning approach*\n",
    "\n",
    "*This article is part of a series that lets a computer play tic-tac-toe using reinforcement learning. The goal is to provide a complete implementation that you can really pick apart and learn reinforcement learning from. It is probably best to read the articles in order. The article including all the code [can be found on Github](https://github.com/PaulHiemstra/minmax_paper/blob/master/min_max_paper.ipynb).*\n",
    "\n",
    "# What exactly is reinforcement learning? \n",
    "Reinforcement learning is the third modeling paradigm next to supervised and unsupervised modeling. It has seen great  successes recently, including [beating the worlds best Go player](https://deepmind.com/research/case-studies/alphago-the-story-so-far). This feat was considered impossible only a few year prior because of the enormous possible moveset in Go that dwarfs even that of chess. I will only briefly touch on how reinforcement learning works, but the following youtube movies should provide a more in depth introduction ([Intro RL](https://youtu.be/0MNVhXEX9to), [Deep RL](https://youtu.be/IUiKAD6cuTA)). \n",
    "\n",
    "Reinforcement learning (RL) is based on the following key concepts:\n",
    "\n",
    "![tree](rl_flow.png)\n",
    "\n",
    "There is an computer agent which takes actions (**A**) that act on environment (**E**). That environment responds by providing a reward (**R**) for that action, and bringing the system to the next state (**S**). Take for example the following state:\n",
    "\n",
    "![tree](tictoe_board.png)\n",
    "\n",
    "Here we consider two options for the next action of the O player: the green and red option. Taking the green option yields a high reward, as we block the X player from winning. Alternatively, the red option yields a low reward as this allows the X player to win. By playing tic-tac-toe many times and receiving rewards the RL agent will slowly learn what action in a particular state provides a good long term **value**. The mapping of state to optimal action is known as a **policy** in RL jargon. \n",
    "\n",
    "# Our first simple minimax algorithm\n",
    "With the basics of RL out of the way, we can now focus on the goal of this article: building an opponent for our RL. Luckily for us, the game of tic-tac-toe is simple enough and can be solved in a brute force manner. This ensures that we end up with the optimal moves in tic-tac-toe, ensuring our RL agent can learn from the best. \n",
    "\n",
    "The main idea is to store all the possible tic-tac-toe states in a tree, and determine for a given state what next move leads to a good result. This is done by recursively searching through the tree and finding rewards. The reward in the subtree below each possible next move informs us which moves is optimal. To illustrate how this works, we start with a very simple tree ([based on this youtube video](https://www.youtube.com/watch?v=5oXyibEgJr0)): \n",
    "\n",
    "![tree](simple_minmax.png)\n",
    "\n",
    "In this game two players compete: one has the goal to maximize the score, one has the goal to minimize the score. First the maximizing player can choose at the top of the tree, then the minimizing player can choose and so on. As the maximizing player our optimal move *seems* to be to go right, that is where the high score of 9 is present. However, the second player will then simply choose 2 when it has the choice. Therefore, in this case the optimal choice is to go left and get a score of three. \n",
    "\n",
    "Solving this kind of tree is done using a [minimax algorithm](https://en.wikipedia.org/wiki/Minimax). At each level in the tree the min or max player makes their optimal choice. If we express this tree in Python we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wooden-classic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "├── \n",
      "│   ├── 3\n",
      "│   └── 5\n",
      "└── \n",
      "    ├── 2\n",
      "    └── 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from treelib import Node, Tree\n",
    "\n",
    "tree = Tree()\n",
    "tree.create_node(\"root\", \"root\")\n",
    "tree.create_node(\"\", \"l1\", parent='root')\n",
    "tree.create_node(\"\", \"r1\", parent='root')\n",
    "tree.create_node(\"3\", \"l1-1\", parent='l1')\n",
    "tree.create_node(\"5\", \"l1-2\", parent='l1')\n",
    "tree.create_node(\"2\", \"r1-1\", parent='r1')\n",
    "tree.create_node(\"9\", \"r1-2\", parent='r1')\n",
    "tree\n",
    "tree.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-looking",
   "metadata": {},
   "source": [
    "where we can solve the minimax problem by recusively going through the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chubby-notice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minmax(tree, current_id, is_max):\n",
    "    '''\n",
    "    tree: tree object to minmax over\n",
    "    current_id: current_id where we are in the tree\n",
    "    is_max: are we the maximizing player?\n",
    "    '''\n",
    "    if tree.depth(current_id) == tree.depth():             # If this holds, we are at the end of the tree\n",
    "        return int(tree[current_id].tag)                   # return the value at the end of tree so it propagates up the tree\n",
    "    children_of_current_id = tree.children(current_id)     # Determine the children of the current node\n",
    "    scores = [minmax(tree, child.identifier, not is_max) for child in children_of_current_id]   # Recursively run this function on each of the children\n",
    "    if is_max:                                             # Return the appropriate score for the max or min player  \n",
    "        return max(scores)\n",
    "    else:\n",
    "        return min(scores)\n",
    "\n",
    "minmax(tree, 'root', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-smile",
   "metadata": {},
   "source": [
    "which indeed shows the correct maximum score the max player can get given this particular tree: 3. Making the recursion explicit in pseudo-code reveals what happens under the hood:\n",
    "    \n",
    "    start-game: minmax('root') -> \n",
    "    max-player: max([minmax('l1'), minmax('r1')]) -> \n",
    "    min-player: max([min([minmax('l1-1), minmax('l1-2')]), \n",
    "                     min([minmax('r1-1'), minmax('r1-1')])]) ->\n",
    "    end-tree  : max([min([3, 5]), \n",
    "                     min([2, 9])]) -> \n",
    "                max([3, 2]) ->\n",
    "                3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-genome",
   "metadata": {},
   "source": [
    "# Minmax tree search on a tic-tac-toe tree\n",
    "We can use the exact same minimax approach for tic-tac-toe using a much larger tree. First we start at the top with nine possible moves for the max player, then we are left with eight moves for the min player, etcetera:\n",
    "\n",
    "![tree](tictoe_tree.png)\n",
    "\n",
    "which leaves us with an enormous tree with all the possible combinations possible on the board. To store the tic-tac-toe state we assign a letter toe each of the fields on the board:\n",
    "\n",
    "![tree](tictoe_letters.png)\n",
    "\n",
    "and create a Python class to store the state, allow updates to the board, check if the game is done, and return the value of our game depending on the outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "atmospheric-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tictoe:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.board = np.zeros(size*size)\n",
    "        self.letters_to_move = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'][:size*size]\n",
    "    def get_board(self):\n",
    "        return self.board.reshape([self.size, self.size])\n",
    "    def make_move(self, who, where, verbose=False):\n",
    "        self.board[self.letters_to_move.index(where)] = who\n",
    "    def get_sums_of_board(self):\n",
    "        local_board = self.get_board()\n",
    "        return np.concatenate([local_board.sum(axis=0),             # columns\n",
    "                               local_board.sum(axis=1),             # rows\n",
    "                               np.trace(local_board),               # diagonal\n",
    "                               np.trace(np.fliplr(local_board))], axis=None)   # other diagonal\n",
    "    def is_endstate(self):\n",
    "        someone_won = len(np.intersect1d((self.size, -self.size), self.get_sums_of_board())) > 0\n",
    "        draw = np.count_nonzero(self.board) == self.size * self.size\n",
    "        return someone_won or draw\n",
    "    def get_value(self):\n",
    "        sums = self.get_sums_of_board()\n",
    "        if self.size in sums:\n",
    "            return 10\n",
    "        elif -self.size in sums:\n",
    "            return -10\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-point",
   "metadata": {},
   "source": [
    "next we recursively create the giant tree and add the board state object at each node. The ID of each node is the sequence of steps taken, e.g. `acig`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def remove_value_list(l, val):\n",
    "    return [el for el in l if el != val]\n",
    "\n",
    "flip_player = {1: -1, -1: 1}\n",
    "\n",
    "possible_options = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\n",
    "\n",
    "def add_options_to_node(tree, node, tt_data, player, remaining_options):\n",
    "    for option in remaining_options:\n",
    "        local_tt_data = copy.deepcopy(tt_data)           # To prevent changing these values in other branches of the tree\n",
    "        local_tt_data.make_move(player, option, False)\n",
    "        if node.identifier != 'root':\n",
    "            new_identifier = node.identifier + option\n",
    "        else:\n",
    "            new_identifier = option\n",
    "        tree.create_node(option, new_identifier, node.identifier, data = local_tt_data)\n",
    "        if len(remaining_options) > 1 and not local_tt_data.is_endstate():  # At end of the game, stop adding nodes\n",
    "            add_options_to_node(tree, tree[new_identifier], local_tt_data, \n",
    "                                flip_player[player], remove_value_list(remaining_options, option))\n",
    "    return None\n",
    "\n",
    "TicToe_state = Tictoe(3)\n",
    "TicToe_3x3 = Tree()\n",
    "TicToe_3x3.create_node(\"root\", \"root\")\n",
    "add_options_to_node(TicToe_3x3, TicToe_3x3[\"root\"], \n",
    "                    TicToe_state, 1, possible_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-geography",
   "metadata": {},
   "source": [
    "With the tree done, we can use minimax code very similar to the simple tree we first started with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "breeding-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_tt(tree, current_id, is_max):\n",
    "    current_node = tree[current_id]                     # Find the tree element we are now\n",
    "    if current_node.data.is_endstate():                 # Are we at the end of the game?\n",
    "        return current_node.data.get_value()            # Return the value\n",
    "    children_of_current_id = tree.children(current_id)  # Determine the children\n",
    "    scores = [minmax_tt(tree, child.identifier, not is_max) for child in children_of_current_id]   # Recursively run this function on each of the children\n",
    "    if is_max:                                          # Return the max or min score depending on which player we are\n",
    "        return max(scores)\n",
    "    else:\n",
    "        return min(scores)\n",
    "    \n",
    "def determine_move(tree, current_id, is_max):\n",
    "    '''\n",
    "    Given a state on the board, what is the best next move? \n",
    "    '''\n",
    "    potential_moves = tree.children(current_id)\n",
    "    moves = [child.identifier[-1] for child in potential_moves]\n",
    "    raw_scores = [minmax_tt(tree, child.identifier, not is_max) for child in potential_moves]\n",
    "    if is_max:\n",
    "        return moves[raw_scores.index(max(raw_scores))]\n",
    "    else:\n",
    "        return moves[raw_scores.index(min(raw_scores))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-coach",
   "metadata": {},
   "source": [
    "Using `determine_move` we can now get the next best move. For example, when the max player plays the `a` square:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "neither-operator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "2.986009120941162\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print(determine_move(TicToe_3x3, 'a', is_max=False))\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-robin",
   "metadata": {},
   "source": [
    "Note that it picks the center `e` square, which is indeed a very strong move in tic-tac-toe. \n",
    "\n",
    "# Solving for slowness\n",
    "So, now we have a worthy opponent for our RL agent. The only problem is that it is very slow, much too slow actually for our RL agent which will have to play a lot of games to learn tic-tac-toe. In part 2 we will use an advanced programming technique to massively speedup the tree search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "tictactoe = Tictoe(3)\n",
    "\n",
    "print('''Welcome to TicTacToe. \n",
    "\n",
    "You can make a move by selecting one of the following letters:''')\n",
    "print(np.array(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']).reshape(3,3))\n",
    "print('''You start, the computer will take the next move\n",
    "\n",
    "Initial board:''')\n",
    "\n",
    "move_history = ''\n",
    "while not tictactoe.is_endstate():\n",
    "    player_move = input('Your move!: ')\n",
    "    tictactoe.make_move(1, player_move)\n",
    "    print(tictactoe.get_board())\n",
    "    move_history += player_move\n",
    "    if tictactoe.is_endstate():\n",
    "        print('You won!...wait you won?????')\n",
    "    \n",
    "    print('Computer is thinking')\n",
    "    computer_move = determine_move(TicToe_3x3, move_history, False)\n",
    "    tictactoe.make_move(-1, computer_move)\n",
    "    print(tictactoe.get_board())\n",
    "    move_history += computer_move\n",
    "    if tictactoe.is_endstate():\n",
    "        print('Computer won!')\n",
    "        \n",
    "    if len(move_history) >= 8 and not tictactoe.is_endstate():\n",
    "        print('Draw...')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
